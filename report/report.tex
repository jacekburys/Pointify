\documentclass{article}
\usepackage{graphicx}
\usepackage{float}
\graphicspath{ {images/} }
\usepackage{hyperref}
\setcounter{tocdepth}{4}
\setlength\parindent{0pt}
\usepackage[margin=1in]{geometry}
\usepackage{multirow}


\newcommand*{\titleGP}{\begingroup % Create the command for including the title page in the document
\centering % Center all text
\vspace*{\baselineskip} % White space at the top of the page

\rule{\textwidth}{1.6pt}\vspace*{-\baselineskip}\vspace*{2pt} % Thick horizontal line
\rule{\textwidth}{0.4pt}\\[\baselineskip] % Thin horizontal line

{\LARGE POINTIFY}\\[0.2\baselineskip] % Title

\rule{\textwidth}{0.4pt}\vspace*{-\baselineskip}\vspace{3.2pt} % Thin horizontal line
\rule{\textwidth}{1.6pt}\\[\baselineskip] % Thick horizontal line

\scshape % Small caps
Tagline 1 \\ 
Tagline 2\par

\vspace*{2\baselineskip}

Created by Group 30\\[\baselineskip]
{\Large Adam Hosier (ah3114)\\ Ayman Moussa (am5514) \\Jacek Burys (jsb314) \\Kabeer Vohra (kv113) \\ Rui Liu (rl2414) \\  \par} % Editor list

\vspace*{1\baselineskip}
{\itshape Imperial College London\par} % Editor affiliation

\vfill 

{\scshape 2017} \\[0.3\baselineskip] % Year published

\endgroup}

\begin{document} 

\titleGP
\thispagestyle{empty}

\newpage
\setcounter{page}{1}
\tableofcontents

\newpage
\section{Executive Summary}

The ability to take an image and generate a 3D point cloud in real time, with multiple calibrated cameras, giving both depth and colour information has multiple use cases today. Through Pointify we have developed a system that utilises Microsoft's proprietary Xbox Kinect hardware to generate these point clouds based on the XYZ and RGB data from the sensors on the devices. It is a truly cross-platform service which allows the camera clients and the server to run on Windows, OS X or Linux operating systems. We achieve this by using services and programming languages that run on all operating systems. Having multiple calibrated Kinect cameras allows for a full 360 degree view of the object or scene that is being photographed.
\\\\
The system works by:
\\\\
Client
\begin{itemize}
\item Uses industry standard open source computer vision library OpenCV to perform matrix operations as efficiently as possible.
\item Uses open source \texttt{libfreenect2} drivers from OpenKinect to extract the colour and depth camera data from the camera.
\item Uses AruCo calibration as a module for OpenCV which we use to detect the marker within the scene and use it to calibrate the co-ordinate spaces of all cameras.
\item Uses open-source C++11 implementation of SocketIO client to communicate with the server and send the point cloud data when requested.
\end{itemize}
~\\
Server:

\begin{itemize}
\item Based on an AngularJS frontend and a NodeJS backend.
\item Run using the Gulp build system to automatically host the server using localhost.
\item Uses SocketIO to calibrate the clients and synchronise the frames when taking a picture or streaming.
\item Uses ThreeJS renderer to display the received point clouds in real-time.
\item Can export current picture or video stream as a PLY file.
\end{itemize}
~\\
For static imaging there are two main use cases. In manufacturing, it is often the case that people wish to create duplicates of objects they already own. For example drill bits or trimmer attachments. Our system can achieve this by taking a picture and then exporting the PLY file which can be immediately imported into AutoCAD to 3D print the object. For medical reasons, the system can create 360 degree images of the human body and then send it off to remote doctors for an online diagnosis.
\\\\
For live imaging there are also two main use cases. For virtual reality gaming it can be useful to be able to map out the scene and work out the distance of the user from certain objects within their room. This could then be used to create live warnings in the virtual reality headset to alert the user when they are about to come into contact with an object in their gaming space. Also much like google street view our system can be used to create a live 3D view of certain points of interest or general roads which can be live streamed over the internet.

\newpage
\section{Introduction}

% TODO : maybe put these side by side
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{placeholder}
  \caption{placeholder}
  \label{fig:placeholder1}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{placeholder}
  \caption{placeholder}
  \label{fig:placeholder2}
\end{figure}

\subsection{Background and Motivation}

A point cloud is a set of data points in some coordinate system. These sets of data can find use in many fields,
for example to create 3D models for VR games, manufacturing, 3D printing and medical applications.
Gathering the points is done using one or more 3D scanners. Microsoft Kinect is one such device which
is widely available and affordable. Pointify is an application that makes use of multiple Kinects to easily
create point clouds which can then be viewed and exported for use in other applications.

\subsection{LiveScan3D}

The initial goal of our project was actually to improve an existing open source application called LiveScan3D.
It has the same purpose as Pointify. Our supervisor, Ben Glocker, proposed that we could
improve the calibration step in LiveScan3D. At that time LiveScan3D was using its own implementation
of marker detection and pose estimation, and it was not making use of the open source libraries like
ArUco or OpenCV. LiveScan3D is only available on Windows and has to be compiled using Visual Studio.
The fact that it is not cross pratform resulted in quite a few difficulties relating to project management.
Furthermore, although LiveScan3D works, we found out that the code was lacking proper software engineering
practices and documentation. We spent almost 2 weeks trying to make progress with the project, but eventually
we decided to pivot and proposed that we could implement our own version that would be cross platform, as well
as incorporating OpenCV and ArUco. This would make the application more flexible for the users
as well as making the product more extensible for developers who may choose to work on different platforms.
By approaching it from the development standpoint rather than a research one, we ensured that our code was readable,
extensible, and used industry-standard and cross-platform drivers, libraries and frameworks.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{livescan}
  \caption{Screenshot of LiveScan3D being run with a single kinect camera}
  \label{fig:livescan}
\end{figure}

\subsection{Objectives}
The first two weeks that we spent working on LiveScan3D gave us a clear understanding of what our application
should do and how we can achieve that. We decided to follow LiveScan3D's idea with one server and multiple
clients sending fragments of the scene to it. However, we decided to add a web application that would be used
to control the clients.
\\\\
The initial aim of the project was to:
\begin{itemize}
  \item Build a distributed system that can gather and merge point cloud data from multiple Kinect cameras.
  \item Be able to display the point clouds in a way that is convenient for the user.
  \item Use standard libraries and frameworks, making the code easy to understand and robust.
  \item Make the project cross-platform (Windows, Linux, OS X).
  % TODO : anything else?
\end{itemize}

\subsection{Achievements}
\begin{itemize}
  \item Distributed system consisting of multiple clients with Kinect cameras, a server and a viewer allowing the user to easily control the Kinect clients and generate the point clouds.
  \item Usage of cross-platform frameworks and drivers allowing the project to be run on Windows, Linux and OS X operating systems.
  \item Exporting the data in the PLY format.
  \item Rendering generated point clouds and stream in real-time
  \item 3D video streaming and recording.
  % TODO : anything else?
\end{itemize}

\newpage
\section{Project Management}
\subsection{Project Plan}
Before we started looking into the specific requirements of this project, we met to discuss how we would go about designing and integrating our code, planning how we would use project management tools, as well as setting specific goals for each two week checkpoint.
\subsubsection{Iteration One}
We planned to spend the first iteration reviewing two codebases to familiarise ourselves with the problem, an existing partial solution, and the libraries we would use to solve this problem. The existing partial solution is a program called LiveScan3D, a research project from Warsaw University of Technology that performed the calibration and streaming elements of the project we were looking for. Our aim was to improve this system, by using a better calibration mechanism, called ArUco, and stream the scene in a way that can be viewed by anyone on any platform. We planned to investigate ArUco, and it's containing library OpenCV, as well as looking in to ways of efficiently streaming data over a network.
\subsubsection{Iteration Two}
We planned to start building our solution in the second iteration, specifically building a client application that could read from the Kinect cameras and display this in a window. We also needed a server application to display a point cloud to the user, as well as placeholder controls to take pictures or start streaming data. We planned to set up our project management techniques and required libraries at the start of this iteration, so they would be ready to use when we started implementing our solution.
\subsubsection{Iteration Three}
We aimed to be able to send a static point cloud from a single camera to the client in the third iteration. This involved converting the data captured from the sensor into a format we could send across the network, and decoding this on the server to display to the user. We anticipated that the setting up of the network components would be troublesome, and we would need some time to experiment with different ways of sending the data, in order to find an efficient method.
\subsubsection{Iteration Four}
In the fourth iteration we aimed to implement calibration, allowing us to connect multiple sensors and stream their captured data to the server simultaneously, with the point clouds they capture aligning. This would involve working with the AruCo library, as well as combining input from multiple sensors together to give us a a view of many point clouds aligning.
\subsubsection{Iteration Five}
We planned to dedicate the final iteration to bug fixing and optimisation. There was a high chance that our final solution would have inefficiencies, as this application is performance critical. We wanted to ensure that we could stream the scene at a pleasant framerate with little to no errors or crashes. This was also a chance to add any small features we had thought of during implementation, as well as clean up the code and work on the user interfaces.
\subsection{Management techniques}
\subsubsection{Version Control}
When working in a team this size, some form of version control is essential. We chose to use git, hosted on GitHub because of our teams' familiarity with the platform. We tried to follow the Git Flow branching model, as our project involved adding many features, one at a time, so following the "branch per feature" practice seemed logical. We mainly worked on our own separate branches, merging to master before a checkpoint, or when an important feature was complete and working. 
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{github}
  \caption{Branch techniques from GitHub}
  \label{fig:github}
\end{figure}
\\
\subsubsection{Task Board}
We found a Kanban-style board very useful to help us manage tasks throughout the project. We chose to use a web platform called Trello for this, as it allowed for all team members to view and edit the board anywhere, as well as providing all the tools we were looking for. We classified each task in to three lists, one labeled "Queued" for tasks that have been planned but not started, then "In progress" for tasks that are currently being worked on, and a list for tasks that had been finished, called "Complete". We would assign these tasks to team members during our meetings, then update the status of the task as we worked on it. We aimed to keep the amount of "In progress" tasks as low as possible, focusing on finishing tasks that were semi-completed before moving on to something new. Trello would automatically notify team members when their tasks had been updated, which allowed us to effectively stay updated with the state of the project between meetings. 
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{trello}
  \caption{Trello project management board}
  \label{fig:trello}
\end{figure}
\\
\subsubsection{Continuous Integration and Deployment}
As our project involved building a client/server style system, we wanted somewhere that was able to constantly run the server to help when developing and demonstrating the tool. We used the Department of Computing's CloudStack instance for this, because of it's easy availability and powerful resources. 
\\\\
We wanted to spend as little time as possible manually running automated tests and following deployment processes, so we decided to use a continuous integration and deployment tool to automate this for us. We chose TeamCity as our tool, as it integrated well with our GitHub repository, and would run in the background of our deployment server. TeamCity would listen for pushes to the master branch of the GitHub repository, then try to build the code and run all automated tests over it. If all these steps were successful, and all the tests passed, we would push the changes to the deployment instance, which would be available to anyone in the college. This proved to be very useful when a single member was testing the functionality of the client software, as it required a connection to the server to run, so having a persistent instance of the server available at all times was essential. 
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.35]{buildserver}
  \caption{TeamCity build and deployment process}
  \label{fig:buildserver}
\end{figure}
\\ 
\subsection{Team Meetings}
As our project involved a lot of interaction between different parts of the code base, we had to meet often in order to discuss how this would work. We aimed to hold formal group meetings weekly, to update the team on our progress and allocate tasks for the next week, as well as meet with our supervisor to demonstrate our progress and listen to his advice. The meetings helped us keep track of what each team member was doing, as well as giving us insight into when specific parts of the project would be completed. We also held more frequent meetings during the week to discuss specific tasks that needed collaboration, and sometimes work on important features together, in a pair programming environment to ensure their quality. 
\subsection{Task Allocation}
After we had investigated the task in more detail, and understood exactly what was needed of us, we were in a position to allocate tasks to group members inside our trello board. The main tasks involved building the client system, the server system, working with the ArUco calibration library and ensuring the software could be built cross platform. We informally assigned team members to these tasks, with each member putting most of their focus on to their task. From these general aims, we would break them down into more specific tasks, such as "Improve frontend playback framerate". We tried to write our tasks such that each of them would give a real benefit to the end user, and improve their experience with the product.
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{trello2}
  \caption{Trello task allocation}
  \label{fig:trello}
\end{figure}
\\ 
\newpage
\subsection{Issues}
\subsubsection{LiveScan3D}
LiveScan3D is run solely on Windows and uses Visual Studio with Microsoft Kinect SDK 2.0. This posed us many problems as a group because only 3 of the members of the group used Windows systems. We also had a problem that with one of the computers, the version of Windows was the 'N' edition of the operating system. Athough we tried the recommended fixes on the internet, we were unable to get LiveScan to run on this version of Windows due to errors about missing MSVCP120.dll and MFPlat.dll. This meant that only 2 computers were able to run the LiveScan software which meant there was only one client and one server running. This did not allow us to test the feature of multiple calibrated cameras and also hindered our development that we were only able to work on 2  computers.
\\\\
We also found it difficult to work with the codebase provided by LiveScan as the quality of the code was poor and did not utilise OpenCV for matrix operations nor did it use any of the leading calibration techniques. This resulted in code that was highly fragile and highly coupled which posed us issues when we were trying to perform tasks like replacing the calibration system.
\subsubsection{Pointify}
When we opted to pivot the project and work on Pointify, we were able to alleviate a lot of the issues that we encountered when working with LiveScan. We did however have issues that we encountered during development. When we initially started development, we needed to start development on a single operating system. We chose to begin development on Linux but this meant that we needed to dual boot our computers or run Linux through a virtual machine. This meant that we were not able to begin development immediately due to the time it took to ensure that our systems were set up correctly. This also posed issues of speed when it came to processing power and networking speeds that were crucial during the development of the system. With virtual machines we were only able to allocate partial amounts of our resources to it, restricting the performance inside the operating system, this was also a problem with networking as the virtual machine uses NAT translation to control the networking within the virtual machine which hindered the network contributing to latency and therefore lag in the streaming. Another problem that we faced with the NAT translation was jitter which caused a delay with each individual packet affecting the obtained frame rate. 
\newpage
There were also hardware issues that we had to deal with. We had designed the system to be able to work with multiple cameras and calibrate them correctly. We however for the project were only provided with two kinect cameras to develop with. For testing with the picture taking, we were able to use the cameras in different positions in order to emulate how the system would be able to deal with multiple cameras. This was a sufficient method to be able to test how the calibration would be able to cope and how the rendering was able to cope with a large volume of points. For live streaming however this proved to be much more difficult. We were unable to test how the system would be able to cope with multiple cameras sending data at once and how this would affect the framerate. The fact that we needed to use the hardware to test the system was also a problem that we faced which meant that we were only able to work on the project when we were in university rather than being able to do any work remotely. For testing purposes we set the build server to be a persistent server for Pointify as well which allowed us to test the client in isolation without having to use a seperate machine to act as the server for our distributed system. We had a problem with USB 3.0 ports as well, some of the computers that we were trying to use did not have USB 3.0 which is required for \texttt{libfreenect2} drivers to be able to communicate with the camera.
\newpage
\section{Design}
We designed the project as a distributed system which has one server and multiple clients. We decided to do it like this to allow for further usability in the future if the user decides to use multiple cameras in a space and also to improve the extensibility of the product for potential new developers who may wish to create different types of clients which is possible with a standard interface.
\subsection{SocketIO}
We decided to use SocketIO to communicate between the clients and the server as it provides a simple to use decorator to the underlying networking code. This allows us to create an abstraction away from the underlying network code as well as allowing a standard interface if in the future alternative clients wish to be developed. SocketIO also has a C++ client as well as a JavaScript client which meant that we did not need to consider the networking protocols for dealing with the sockets between the server and the client. When calibrating, the server simply sends a calibration request to the client which then sends back a boolean callback if the calibration was successful or not. When streaming, the server simply requests a picture from each client every frame and synchronises the recieved frames itself so that all the client needs to do is implement one function to take a picture and does not need to know whether it is streaming or not. The server also sends start streaming and stop streaming notifications over SocketIO so that the client can change its state when streaming if required. This is done to aid further extensibility.
\subsection{Distributed System}
We chose to design our system as a distributed system for a few main reasons. Using a distributed system is good for applications like these because when adding more devices to the system, you are also adding more computers and computing power. This means that increased number of cameras has a negligible increase on the load of the server which makes the system more powerful. Another advantage of this design is that the cameras do not need to be physically connected to a single computer and can be situated in different corners of the space. This is a vital feature for some of the use cases of our system, such as imaging a point of interest which would require the cameras to be quite far from each other. There are also drawbacks of the distributed approach however. Having a computer dedicated to each camera means that the hardware requirements are greater and having the system run over the network means that the framerate and latency will be worse than having it all run on one server.
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.35]{distributed}
  \caption{Diagram of the distributed system design of the project}
\end{figure}
\newpage
\subsection{Client}
The UML sequence diagram of the client's structure is shown below:
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.6]{clientUML}
  \caption{UML sequence diagram of the client}
\end{figure}
\subsubsection{C++}
\subsubsection{ArUco}
ArUco was the calibration system that was proposed by our supervisor Ben Glocker. It is the current leading calibration system and is provided as a module for OpenCV 2. Using standard libraries over implementing it ourselves reduces the possibility of bugs and the development cost. Standard libraries also makes our code easier to read for future developers to extend. It is also faster than AprilTags, which is the other calibration system.
\subsubsection{OpenCV 2}
OpenCV is the industry standard open source library for computer vision. It can be run in C++, C, Python and Java for Windows, Linux, OS X, iOS and Android. OpenCV is very optimised for efficiency due to its large developer base and this performace is essential for us since we use it on every loop of our camera. It can also take advantage of multi-core processing and since almost all devices are multi-core now hence this improves the performance of our application. Since it uses OpenCL, it can take advantage of hardware acceleration to compute the matrix operations efficiently.
\subsubsection{Libfreenect2}
Libfreenect2 was our only option when it came to choosing a cross-platform driver to use for communicating with the Kinect camera. Microsoft only supplied the Kinect SDK 2.0 for Windows. It utilises OpenGL and OpenCL libraries for depth processing. The system also supports up to 5 devices so it allows extensibility in the future if someone wishes to use a single computer to host multiple Kinect cameras.
\subsubsection{CMake}
Our tool of choice for building the client was CMake, as it is a well established tool for building cross platform applications. CMake allowed us to write one make script "CMakeLists.txt", which compiled to a build solution that would run on the user's platform. On Unix systems this would be a Makefile, which allowed to user to use "make" to build the application, and on Windows machines this compiled a Visual Studio solution which could be built inside Visual Studio. We had to make sure that the libraries we used were built for the platform that the client was running on. This could be done by building them from source inside our CMake file, or providing the users with precompiled binaries for them. For our needs we found it easier to compile the libraries to the platforms we were using, and downloading them onto the machines we worked on. 
\subsubsection{Data}
We chose to do the calibration on the server and send the point clouds directly to the client to allow for distributed computation and speed up the transfer of data.
\newpage
\subsection{Server}
The UML sequence diagram of the server's structure is shown below:
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.6]{serverUML}
  \caption{UML sequence diagram of the server}
\end{figure}
\subsubsection{Gulp} %webapp
\subsubsection{AngularJS}

\newpage
\section{Implementation}
\subsection{Calibration}
\subsubsection{Marker pose estimation}
When streaming with more than one camera, we required the point clouds they send to be aligned such that they can be displayed seamlessly on top of each other, building a 3D representation of the scene. The idea behind this involves applying a transformation to each point in the scene, to bring it from the camera's perspective to some common perspective shared by all the cameras. We discussed two ways of doing this, one of which chooses one of the cameras as the "master" camera, then synchronises the other cameras around it. The alternative method is by centering all of the cameras on a common point, in our case the calibration marker we use. An advantage of using the first method is that one less camera would need to be calibrated, as all of the other cameras would be centered around the master, but this would lead to more complicated code, and more difficult communication between the cameras. The second approach would give more elegant and maintainable code, as each of the cameras performs the same calibration, but sacrificing a small amount of efficiency on one of the cameras. For these reasons, we chose to use the second approach, calibrating all cameras around the marker placed in the scene. \\
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.6]{aruco}
  \caption{Example ArUco marker}
\end{figure} \\
This involved working with a third party library called ArUco, which is a well established computer vision library for C++ that helps with the calibration process. In the method we chose, the cameras are all centered around a stationary marker (similar to Figure \ref{fig:aruco}) placed in the scene, so we required ArUco to detect these markers and calculate the camera's position relative to the marker. The aim is to find the transformation that brings points centered around the camera, into a space with the marker as the origin, then once this is calculated we can apply this transformation to each point cloud to align them. We start this process by detecting any markers visible in the scene, then finding rotation and translation vectors from the camera to this marker, adjusting for intrinsic camera parameters such as focal length and principal point offset. From this we can build up a transformation matrix to combine both the rotation and translation, before inverting it to get the translation from camera to marker as shown in Figure \ref{fig:transformationMatrix}. \\
\begin{figure}[h]
  \[\left(\begin{array}{cccc}
      & | &   & | \\
    - & r & - & t \\ 
      & | &   & | \\
    0 & 0 & 0 & 1
    \end{array}\right)^{-1}\]
  \caption{Transformation matrix, where \textbf{r} is the rotation and \textbf{t} is the translation}
  \label{fig:transformationMatrix}
\end{figure}
\subsubsection{Point cloud transformation}
Once we have found the transformation matrix as in the previous part, we must apply it to each point cloud before they are is sent to the server. Each xyz point must be extended with a 1 on the bottom row, so that it can be multiplied correctly with the transformation matrix. We experimented applying the transformation matrix to each point to give the transformed point, but this was causing some performance issues due to the amount of points needed to be transformed at once. A better way of doing this would be to apply the transformation to all points in one operation with a more complex matrix operation, shown in Figure \ref{fig:transformationMatrix}. We used an optimised implementation of this in opencv called perspectiveTransform to do this efficiently, which when given the set of input points, and the transformation function we calculated earlier, would return the transformed points we needed. When each client calculated the transformation to bring their origin to the marker, and applied this transformation to the point clouds captured from the connect, they could be successfully combined such that they would align.\\
\begin{figure}[h]
  \[\left(\begin{array}{cccc}
      & | &   & | \\
    - & r & - & t \\ 
      & | &   & | \\
    0 & 0 & 0 & 1
    \end{array}\right)^{-1}
  \left(\begin{array}{cccc}
    \multirow{3}{*}{\textbf{$p_1$}} & \multirow{3}{*}{\textbf{$p_2$}} & \multirow{3}{*}{\dots} & \multirow{3}{*}{\textbf{$p_n$}} \\
    & & & \\
    & & & \\
    1 & 1 & & 1
    \end{array}\right)\]
  \caption{Transformation matrix as before, applied to each point $p_1$ \dots $p_n$}
  \label{fig:transformationApplication}
\end{figure}
\subsection{Point cloud streaming}
Once we had built the point cloud, the next step was to send the information over a network to the server. We decided to do this by packing all the points into a binary buffer, as there would be no wasted space. This information we needed to send consisted of three 32 bit floats, for x y and z coordinates, as well as three 8 bit integers for the rgb colour values, giving a total of 15 bytes per point. There can be up to 512 x 424 points in space, giving a maximum total buffer size of 3180KB per frame. In reality, this number was much smaller, as the sensor can not pick up points that are too close or far away. We discussed the idea of compressing this buffer to reduce transfer speed, but as our client software was reletively CPU intensive already, we decided to send the uncompressed data instead.
\newpage
\subsection{Challenges}
\subsubsection{Dual Camera Sensors}
When performing the calibration we noticed that the calibration was slightly off. After doing some research we worked out that this was caused by the offset between the depth and rgb cameras on the Kinect hardware. The reason for this was that we were performing the AruCo calibration on the rgb matrix from the colour camera and then we were using the obtained rotation and translation vectors to calibrate the output from the depth camera. This slight offset caused the obtained origin in the coordinate space to be different and then caused the point clouds obtained from the different cameras to not align correctly. This is demonstrated in figure~\ref{fig:rgbdepth} below.
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4]{rgbdepth}
  \caption{Showing the offset between the field of view of rgb and depth cameras on the Kinect}
  \label{fig:rgbdepth}
\end{figure}
This produces an offset in the calibration as shown in figure~\ref{fig:calibrationoffset} below.
\\
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.26]{registeredunregisteredpointcloud}
  \caption{Showing the difference between the calibration accuracy of registered and unregistered}
  \label{fig:calibrationoffset}
\end{figure}
\\
To fix this issue we needed to use the registered camera matrix from the \texttt{apply} function within \texttt{libfreenect2}. This gave us a matrix which provided the rgb data points mapped onto the corresponding points from the depth camera's field of view. We therefore performed the calibration from the depth cameras field of view and output the correct resultant rotation and translation vectors for the point cloud generated from this camera.
\newpage
\subsubsection{Calibration Accuracy}
When using the registered matrix, we encountered a much worse camera matrix than the one directly taken from the rgb camera as shown in figure~\ref{fig:registeredaccuracy} below.
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.7]{registeredunregisteredview}
  \caption{Showing the difference between the registered and rgb matrices}
  \label{fig:registeredaccuracy}
\end{figure}
\\
This reduced quality meant that AruCo was unable to accurately detect the markers in the scene. To rectify this we noticed that where \texttt{libfreenect2} was unable to detect the marker accurately, there were very small parts of the screen that were being detected as a marker as well as the main marker. This caused us issues as with multiple markers on the screen, when performing the calibration, AruCo may take the wrong marker to calibrate on, causing the points to be calibrated on the wrong location. To rectify this, during the calibration step, if there were more than one marker in the scene we would calculate the size of the 3D planes and then keep only the largest plane and remove the smaller ones. This would then return us with only the main marker in the scene if it was detected correctly.
\subsection{Unsolved Challenges}
There were also a few unsolved challenges that we face with the project that we have been unable to find a solution to at this time. These challenges that we face are related to the streaming capability of our system and they are related to the obtainable frame rate of the streaming.
\subsubsection{TCP vs UDP}
With the network we had a major problem with the protocols of TCP vs UDP. Since we are streaming the frames, UDP is a much more desirable protocol to be using to increase the framerate of our system. The first problem we encountered with using this is that SocketIO only supports TCP communication. We then considered using standard networking protocols rather than SocketIO for the streaming of the frames between the client and the server but we realised that, for the server to communicate between its nodeJS backend and its angularJS frontend, it needs to use JavaScript for the rendering of the point clouds. JavaScript as a language does not natively support UDP therefore we realised that even if we would be able to communicate the server and the client, we would encounter a bottleneck between the backend and the frontend of the server and experience no additional improvement on the framerate.
\subsubsection{Synchronisation Lag}
Lag is another issue we encountered. We were able to obtain a moderate framerate between the server and the client for streaming but we experienced on average a 3-second delay between the camera display on the client and the server. We have not encountered any bottlenecks within our system that could be causing this lag and we are assuming that it is simply the network delay that is causing it.
\subsubsection{Socket.IO C++ Client}
The TCP based streaming protocol that Socket.IO uses has not been implemented with the current C++ client offered.
This meant that during streaming of the frames we had to send our data via standard \texttt{emit} messages instead. We are not sure as to whether this would have made a significant impact on the framerate but the data sent over the normal protocol has a much larger header per frame.
\newpage
\section{Evaluation}
\subsection{Design}

\subsection{Code Quality}
Overall we are fairly happy with the quality of the code we have produced. We approached the code in an object oriented fashion and we have clear communication between the various components of our code. One benefit of the amount of pair programming we did is the peer evaluation of the code as you type which holds the standard to a higher level. We also did not experience more bugs appearing as we progressed throughout the project or did we experience a vastly increased development cost which are indications of low fragility and low rigidity. We also agreed a level of code quality at the start that has been adhered to throughout the project.
\\\\
We utilised branches effectively for each feature of the product so that we could then be comfortable about the quality of any code on a seperate branch and the fact that it is working correctly before merging into the master branch. This way we know that the master branch is the release branch of everything that is fully working and if there are issues on other branches we can use the master branch to test with slightly older versions of the code.
\\\\
If we had more time and were pursuing a slower but higher quality level of development we would have done more peer reviewing of each commit before accepting it. We decided not to waste time doing this due to the agile development structure of the project and the fact that we had to have an extensive set of features for the presentation.
\subsection{Calibration Accuracy}
As discussed earlier in the project, the calibration accuracy was a topic that had to undergo certain levels of compromise in order to get it fully working.
\\\\
One of the major causes of issues with ArUco was the fact that we had to calibrate on the registered matrix rather than the direct camera matrix. This was much less reliable and we often had issues of the marker not being detected at all even though it was in the scene. We managed to eliminate the issues with ArUco detecting ghost markers in the scene but we still suffered with the basic detection of the marker.
\\\\
We tried to improve this by reducing some of the control factors within the scene. We tried to make the marker bigger by maximising the space on the A4 sheet of paper that the marker could occupy. We stuck the marker onto a flat object so that we could eliminate the curve in the marker that we noticed caused a significant decrease in the accuracy of the calibration. We then tried to use different printers for the marker as we noticed that the printers supplied at university used a shiny ink and light which was shining directly on the marker caused parts of the marker to be blanked out with respect to the camera and then caused it to not be detected. We also then tried to make the marker much bigger using an A3 sheet of paper.
\\\\
We noticed that the last suggestion seemed to work the best and gave us the most reliable detection of the marker within the scene. We were not extremely satisfied with the quality but we feel that with the time we have available and the restrictions on the hardware, this is the best we can achieve. We also feel that if we were using a camera where the depth sensor and the camera are close enough to each other that the field of view would not be different then we would be able to obtain a much higher quality of calibration.
\\\\
Once we had ArUco detect the marker in the scene and calibrate successfully, we noticed that the rotation and translation vectors that we were able to obtain were very accurate. We were able to stream and take pictures with multiple cameras and have the point clouds align almost seamlessly. We feel that the calibration is extremely fast and also if the right conditions are provided can also detect the marker fairly well. We realise that we will not always have optimal conditions for some of our use cases and this is something that we would have to look at for further work.
\\\\
(image of two point clouds aligned and calibrated)
\subsection{Performance}
Both the client and the server seemed to run well on all of the systems we tested on. There were certain aspects of both that caused the running performance to be affected.
\\\\
In the client we had a loop that was constantly running until the application was closed. It was also registered to many callbacks in Socket.IO that it was listening on the entire time. In the main loop of the client we were obtaining the latest frame from the kinect camera and simply displaying it for the user to see using the OpenCV \texttt{imshow}. This created a live view of the camera for the user of the client to see when it was running. With the ArUco calibration we also decided to add some elements of the calibration to each loop of the camera which affected the performance. We wanted to display a bounding box whenever a marker was detected and send live updates on how many markers there are in the scene back to the server. Both these things negatively affected the client but we feel that the speed is still fairly fast and the improvement in usability is worth the cost.
\\\\
For the server, there are very few elements that happen in real-time when it is not streaming that are different from a standard web application. The server has a front end which dynamically updates whenever the server sends it a message over Socket.IO. We use ThreeJS to render the points which is a very lightweight framework. Apart from the rendering, the server has buttons which send Socket.IO messages but there is not much else that runs constantly when idle. Therefore the server is just as fast as any other web interface and perfectly usable.
\\\\
Overall we feel that the performance of the server and the client are perfectly fast and we have not experienced any noticable decrease with any of our systems.
\subsection{Testing}
\subsection{Build Server}
The build server helped us massively in the process of testing and running demos. It allowed us to have a persistent instance of the server code live and accessable at all times, allowing a single programmer to work on the client code, and test it against the server code. It would seamlessly update whenever a commit was pushed to the master branch, run build checks and unit tests before deploying the code live. Any of us could log in to the TeamCity instance to check the build status and manually run tests, allowing us to be updated with the status of the project easily.
\\\\
A downside of the setup we used is that it was set up to only observe the master branch, which was only updated for major milestones. There were times where we would have liked the server to be observing another branch that was more often updated to test against. Another issue with our setup was how we used the departments cloudstack setup, which is only accessable from within college. This meant that whenever we wanted to access the build server from outside of college, we had to take a more convoluted approach, such as using the college VPN.
\subsection{Deployment} %gulp
\subsection{Streaming}
The streaming was the section we spent most of the time improving towards the end of the project. This was a challenge since we are trying to process, send and render multiple point clouds per second over the network. We found this part the most difficult to test effectively as the load on the network was such a large factor that we would often experience false positives as to whether a certain improvement we had implemented had improved or worsened the obtained framerate. To negate these issues, towards the end of the project, we opted to create a local network to remove this control variable from our testing. We also used LiveScan3D to compare to what framerate we should theoretically be able to achieve with their systems and what we were achieving, all the while taking into account the extra overhead we had with respect to LiveScan3D.
\\\\
One of the first things we attempted to reduce the overhead is to reduce the size of the data being sent. When sending individual points over Socket.IO, we were trying to send each point in its own seperate message. We discovered after looking into Socket.IO that we were not utilising all available space in each message that was being sent and we were sending extra blank bits over the network. To solve this problem we decided to pack all of the points into a string buffer for each frame and send the entire frame as one string message. We found that this considerably improved the framerate that we were experiencing. The second improvement that we made is that we tried to pre-cache the frame in the client during streaming so that when a new frame was requested, the client did not need to process the frame, it could simply send the latest processed frame immediately. We found that this had a slight improvement on the streaming but also had a slight negative impact on the speed of the client as it needed to perform more computation in each loop. The third improvement that we made was to try and request a frame every time the server had finished processing the frame which would avoid the network getting flooded. We realised with this approach that it actually significantly slowed down the framerate since the processing time of each frame was more of a cost than the network being flooded. We decided to implement the first 3 changes and we feel this is sufficient for this stage of the project.
(maybe also talk about trying to spawn a load of threads to send the data on the client)
\\\\
We also noticed that we were experiencing a considerable amount of lag between when something happened in the client and when it was to be displayed on the server. We also noticed that this lag was getting worse as the streaming duration increased. This immediately gave us the impression that there was a queue somewhere within the system that was holding the frames that needed to be processed and this queue was getting larger because the frames were being produced faster than they were being rendered. To rectify this we decided to store the latest recieved frame for each camera in the server and only process the one that we have stored on each iteration of the renderer. This worked well in keeping the lag to be consistent during the streaming and not increasing. We did find however that there was still a constant level of lag that we experienced of around 3 seconds which we were unable to reduce further. Hopefully in the future we would be able to come up with a better way of reducing lag that could work better.
\\\\
Another problem we encountered with the streaming was the time synchronisation of multiple cameras within the scene. What we found is that when we were testing with different computers that all had different hardware specifications that one of the clients would be considerably faster than the other and the two point clouds in the same frame would be showing data at different points in time. To rectify this, we decided to make the server only request a new frame from each client once all of the clients had sent their frame. We were not concerned about the clients not sending their frame because if a client disconnects then it is removed from the client pool and the server will not wait for it. We found that this worked well and we were able to keep all of the frames in a time sync. The main problem we found with this was that we would have to wait for the slowest client for each frame. This meant that the streaming was now as slow as its slowest member. We feel that this is sufficient for the current state of the system but we would probably like to implement some more sophisticated synchronisation systems if we were to have more time in the future.
\\\\
Overall we found that the streaming was the hardest section to improve on and after dedicating a considerable amount of time to improving it, we feel that this is the most improvement that we were able to achieve. When working on it in the future we think this is the biggest candidate for improvement.
\newpage
\section{Conclusion and Future Extensions}
\subsection{Conclusion}

When we received the proposal for 3D reconstruction, even though it was our last choice, we were nevertheless extremely excited to develop the application despite most of our members having no experience in using OpenCV. It was also a great opportunity to utilise the Computer Vision material for those in group who were taking the course because we were able to use the techniques taught and practically apply them in the creation of Pointify. Background research into 3D reconstruction gave us a general idea of what we were aiming to create as we did not fully understand the concept of 3D reconstruction before. From the start, ensuring software development practices such as pair programming were maintained throughout the project was one of the primary importances as members were able to discuss their problems together and discuss how they could better implement a solution.
\\\\
The technical aspect of creating Pointify (coming from the idea that an object or a scene can be reconstructed by rendering many points together from captured depth information) took the most time during the 3 months. During this short time we have endured and have managed to produce a stable application, that combined various technologies together to successfully capture depth information through multiple Kinect cameras, perform calibration and then render the many points together to 3D reconstruct a scene. In order to start we had to understand how LiveScan3D worked which took some time as we felt that LiveScan3D's code was poorly structured. However after analysing LiveScan3D's implementation, several methods were proposed through our group meeting and then a final concrete solution was decided. We then were able to carefully plan out the major sections of the project to which each group member was assigned a task.
\\\\
For some members of the group, it was their first time to use angularJS and nodeJS so they were able to acquire a new skill as they used it for developing the front and backend. Although the main focus of the project was ensuring that we completed implementing the features at each iteration, the latter part of the project was spent on debugging and trying to increase the current frame rate from streaming point clouds. To increase the frame rate we tried to stream the point clouds by connecting the server and client through an ethernet hub rather than wifi however this only a small increase in the frame rate. Nevertheless, the small increase in the frame rate was noticable enough. 
\\\\
Given the chance to start the project again from the beginning, we would have gone for creating the project on Linux straightaway, as it is easier to develop on there, after experiencing first hand the challenges of building the project on Windows as some time we lost was spent trying setup OpenCV. Even though we took a small risk to start afresh and develop Pointify on Linux for a cross platform application, it paid off it the end. We feel we underestimated the complexity of the calibration of the Kinect cameras and did not consider the offset between the depth and rgb camera until our supervisor, Ben, pointed it out to us. It would have been more effective and time efficient to meticulously plan the calibration aspect of the project as it was the most time consuming aspect of the development.
\\\\
Aligning all the times members were able to work together was tricky due to the courses taken by each member. This meant that during some group meetings, some members would not be present however this did not pose a problem because members were kept updated on the topics discussed in the meeting. Overall, we are happy about the project we created over term even though we would have liked to have some of the feature extensions integrated. We have decided to release it as an open source project on github with detailed instructions on how to build, test and develop Pointify. It would be nice if we could get feedback through reported bugs or possible improvements as this would allow us to improve Pointify and learn what could have been done better. We also hope that our project might encourage other developers to use Pointify in their own projects. 
%As various challenges e.g. with differences in
%coordinate systems used internally by our devices and the OpenCV library code have %shown us, it would
%be wiser to first plan the procedure with finer detail. Moreover, we have been %overly focused on correctness
%of the system for a while and neglected the efficiency of the calibration phase. We %have experienced firsthand
%that the usability is crucial as the compression techniques (e.g. using 
\newpage
\subsection{Future Extensions}
There are features that we believe can improve the usability of this product, but didn't implement due to the time constraint on the project:
%1. GUI implementation%
\subsubsection{GUI Implementation}
Currently, only the server has a GUI implementation in place but the client side has no GUI implementation as everything is done by command line. As it stands now, the command line outputs non-user friendly errors to the client. We can try to build up a user friendly GUI implementation on the client side to ensure a smoother client experience. The current client application only opens up the camera view for the Kinect and does not display any other helpful information. We can add to the display, the current frame rate (when streaming) and the calibration status of the client so it does not have to check on the server machine the status (which is useful if server and client are not in the same room). We can try to use some platform-independent UI library to achieve this, possibly Qt.
%2. Pointify to support other camera devices%
\subsubsection{Pointify to support other camera devices}
Modifying Pointify to work not only with Kinect cameras, but with many more types of different cameras. The disadvantage of using Kinect cameras is that they must be plugged into a power socket which constrains the mobility of the client should they want to roam about. There are other cameras, that do not require to be plugged in to a power socket, which can capture depth information so it is possible to configure Pointify to work for these cameras. What we can do is to build up a layer of indirection on the hardware driver, so that the current existing calibration code will work with different camera implementations. We would also need to unify the data format that is passed so that we can maximise the amount of code that can be reused.
\subsubsection{Pointify installation}
In order to get Pointify to run, several prerequsites must be installed such as \texttt{Bower}, \texttt{Ruby} and \texttt{MongoDB}. With a proper simple installation set in place, we can run a single script to detect and install the aforementioned prerequsites instead of manually installing them one by one. After the prerequsites are installed, further dependancies must be installed, the \texttt{mongod} daemon must be set up in a new shell and then finally the server started by calling \texttt{gulp serve}. The installation script would then install these dependancies and then run the daemon before starting the server in a browser window.
\subsubsection{Recordings}
A very nice feature which would go with Pointify is the ability to record point cloud streams. After successfully calibrating the clients, you can then press the "Start recording" button to record a point cloud stream which then later can be played back. The server should be able save and load these point cloud streams that it records in a database. Although each frame in size is roughly a few megabytes, \texttt{MongoDB} should be suitable to storing the recording point cloud streams. (Add more here)

% TODO : references

\end{document}
