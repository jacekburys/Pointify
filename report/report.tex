\documentclass{article}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{hyperref}
\setcounter{tocdepth}{4}
\usepackage[margin=1in]{geometry}

\newcommand*{\titleGP}{\begingroup % Create the command for including the title page in the document
\centering % Center all text
\vspace*{\baselineskip} % White space at the top of the page

\rule{\textwidth}{1.6pt}\vspace*{-\baselineskip}\vspace*{2pt} % Thick horizontal line
\rule{\textwidth}{0.4pt}\\[\baselineskip] % Thin horizontal line

{\LARGE POINTIFY}\\[0.2\baselineskip] % Title

\rule{\textwidth}{0.4pt}\vspace*{-\baselineskip}\vspace{3.2pt} % Thin horizontal line
\rule{\textwidth}{1.6pt}\\[\baselineskip] % Thick horizontal line

\scshape % Small caps
Tagline 1 \\ 
Tagline 2\par

\vspace*{2\baselineskip}

Created by \\[\baselineskip]
{\Large Jacek Burys \\ Kabeer Vohra \\ Adam Hosier \\ Rui Liu \\ Ayman Moussa \par} % Editor list

\vspace*{1\baselineskip}
{\itshape Imperial College London\par} % Editor affiliation

\vfill 

{\scshape 2016} \\[0.3\baselineskip] % Year published

\endgroup}

\begin{document} 

\titleGP
\thispagestyle{empty}

\newpage
\tableofcontents
\thispagestyle{empty}

\newpage
\setcounter{page}{1}
\section{Executive Summary}

\newpage
\section{Introduction}

% TODO : maybe put these side by side
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{placeholder}
  \caption{placeholder}
  \label{fig:placeholder1}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{placeholder}
  \caption{placeholder}
  \label{fig:placeholder2}
\end{figure}

\subsection{Background and Motivation}

A point cloud is a set of data points in some coordinate system. These sets of data can find use in many fields,
for example to create 3D models for VR games, manufacturing, 3D printing and medical applications.
Gathering the points is done using one or more 3D scanners. Microsoft Kinect is one such device which
is widely available and affordable. Pointify is an application that makes use of multiple Kinects to easily
create point clouds which can then be viewed and exported for use in other applications.

\subsection{LiveScan3D}

The initial goal of our project was actually to improve an existing open source application called LiveScan3D.
It has the same purpose as Pointify. Our supervisor, Ben Glocker, proposed that we could
improve the calibration step in LiveScan3D. As that time LiveScan3D was using its own implementation
of marker detection and pose estimation, and it was not making use of the open source libraries like
ArUco or OpenCV. LiveScan3D is only available on Windows and has to be compiled using Visual Studio.
The fact that it is not cross pratform resulted in quite a few difficulties relating to project management.
Furthermore, although LiveScan3D works, we found out that the code was lacking proper software engineering
practices and documentation. We spent almost 2 weeks trying to make progress with the project, but eventually
we decided to pivot and proposed that we could implement our own version that would be cross platform, as well
as incorporating OpenCV and ArUco. This would make the application more flexible for the users
as well as making the product more extensible for developers who may choose to work on different platforms.
By approaching it from the development standpoint rather than a research one, we ensured that our code was readable,
extensible, and used industry-standard and cross-platform drivers, libraries and frameworks.

\subsection{Objectives}
The first two weeks that we spent working on LiveScan3D gave us a clear understanding of what our application
should do and how we can achieve that. We decided to follow LiveScan3D's idea with one server and multiple
clients sending fragments of the scene to it. However, we decided to add a webapp that would be used
to control the clients.
\begin{itemize}
  \item Build a distributed system that can gather and merge point cloud data from multiple Kinect cameras.
  \item Be able to display the point clouds in a way that is convinient for the user.
  \item Use standard libraries and frameworks, making the code easy to understand and robust.
  \item Make the project cross-platform (Windows, Linux, Mac).
  % TODO : anything else?
\end{itemize}

\subsection{Achievements}
\begin{itemize}
  \item Distributed system consisting of multiple clients with Kinect cameras, a server and a viewer allowing the user to easily control the Kinect clients and generate the point clouds.
  \item Usage of cross-platform frameworks and drivers allowing the project to be run on Windows, Linux and Mac operating systems.
  \item Exporting the data in PLY and STL file formats.
  \item 3D video streaming and recording.
  % TODO : anything else?
\end{itemize}

\newpage
\section{Project Management}
\subsection{Project Plan}
Before we started looking into the specific requirements of this project, we met to discuss how we would go about designing and integrating our code, planning how we would use project management tools, as well as setting specific goals for each two week checkpoint.
\subsubsection{Iteration One}
We planned to spend the first iteration reviewing two codebases to familiarise ourselves with the problem, an existing partial solution, and the libraries we would use to solve this problem. The existing partial solution is a program called LiveScan3D, a research project from Warsaw University of Technology that performed the calibration and streaming elements of the project we were looking for. Our aim was to improve this system, by using a better calibration mechanism, called ArUco, and stream the scene in a way that can be viewed by anyone on any platform. We planned to investigate ArUco, and it's containing library OpenCV, as well as looking in to ways of efficiently streaming data over a network.
\subsubsection{Iteration Two}
We planned to start building our solution in the second iteration, specifically building a client application that could read from the Kinect cameras, and a server application to display a point cloud to the user. We planned to set up our project management techniques at the start of this iteration, so they would be ready to use when we started implementing our solution.
\subsubsection{Iteration Three}
We aimed to be able to stream a point cloud from a single camera to the client in the third iteration. This involved converting the data captured from the sensor into a format we could send across the network, and decoding this on the server to display to the user. 
\subsubsection{Iteration Four}
In the fourth iteration we aimed to implement calibration, allowing us to connect multiple sensors and stream their captured data to the server simultaneously, with the point clouds they capture aligning.
\subsubsection{Iteration Five}
We planned to dedicate the final iteration to bug fixing and optimisation. There was a high chance that our final solution would have inefficiencies, as this application is performance critical. We wanted to ensure that we could stream the scene at a pleasant framerate with little to no errors or crashes.
\subsection{Management techniques}
\subsubsection{Version Control}
When working in a team this size, some form of version control is essential. We chose to use git, hosted on GitHub because of our teams' familiarity with the platform. We tried to follow the Git Flow branching model, as our project involved adding many features, one at a time, so following the "branch per feature" practice seemed logical. We mainly worked on our own separate branches, merging to master before a checkpoint, or when an important feature was complete and working.  
\subsubsection{Task Board}
We found a Kanban-style board very useful to help us manage tasks throughout the project. We chose to use a web platform called Trello for this, as it allowed for all team members to view and edit the board anywhere, as well as providing all the tools we were looking for. We classified each task in to three lists, one labeled "Queued" for tasks that have been planned but not started, then "In progress" for tasks that are currently being worked on, and a list for tasks that had been finished, called "Complete". We would assign these tasks to team members during our meetings, then update the status of the task as we worked on it. We aimed to keep the amount of "In progress" tasks as low as possible, focusing on finishing tasks that were semi-completed before moving on to something new. Trello would automatically notify team members when their tasks had been updated, which allowed us to effectively stay updated with the state of the project between meetings. 
\subsubsection{Continuous Integration and Deployment}
As our project involved building a client/server style system, we wanted somewhere that was able to constantly run the server to help when developing and demonstrating the tool. We used the Department of Computing's CloudStack instance for this, because of it's easy availability and powerful resources. \\
We wanted to spend as little time as possible manually running automated tests and following deployment processes, so decided to use a continuous integration and deployment tool to automate this for us. We chose TeamCity as our tool, as it integrated well with our GitHub repository, and would run in the background of our deployment server. TeamCity would listen for pushes to the master branch of the GitHub repository, then try to build the code and run all automated tests over it. If all these steps were successful, and all the tests passed, we would push the changes to the deployment instance, which would be available to anyone in the college. This proved to be very useful when a single member was testing the functionality of the client software, as it required a connection to the server to run, so having a persistent instance of the server available at all times was essential.  
\subsection{Team Meetings}
As our project involved a lot of interaction between different parts of the code base, we had to meet often in order to discuss how this would work. We aimed to hold formal group meetings weekly, to update the team on our progress and allocate tasks for the next week, as well as meet with our supervisor to demonstrate our progress and listen to his advice. The meetings helped us keep track of what each team member was doing, as well as giving us insight into when specific parts of the project would be completed. We also held more frequent meetings during the week to discuss specific tasks that needed collaboration, and sometimes work on important features together, in a pair programming environment to ensure their quality. 
\subsection{Task Allocation}
After we had investigated the task in more detail, and understood exactly what was needed of us, we were in a position to allocate tasks to group members. The main tasks involved building the client system, the server system, working with the ArUco calibration library and ensuring the software could be built cross platform. We informally assigned team members to these tasks, with each member putting most of their focus on to their task. From these general aims, we would break them down into more specific tasks, such as "Improve frontend playback framerate". We tried to write our tasks such that each of them would give a real benefit to the end user, and improve their experience with the product. 

\newpage
\section{Design}

\newpage
\section{Implementation}
\subsection{Calibration}
\subsubsection{Marker position estimation}
When streaming with more than one camera, we required the point clouds they send to be aligned such that they can be displayed seamlessly on top of each other, building a 3 dimensional representation of the scene. The idea behind this involves applying a transformation to each point in the scene, to bring it from the camera's perspective to some common perspective shared by all the cameras. We discussed two ways of doing this, one of which chooses one of the cameras as the "master" camera, then synchronises the other cameras around it. The alternative method is by centering all of the cameras on a common point, in our case the calibration marker we use. An advantage of using the first method is that one less camera would need to be calibrated, as all of the other cameras would be centered around the master, but this would lead to more complicated code, and more difficult communication between the cameras. The second approach would give more elegant and maintainable code, as each of the cameras performs the same calibration, but sacrificing a small amount of efficiency on one of the cameras. For these reasons, we chose to use the second approach, calibrating all cameras around the marker placed in the scene.
\\
This involved working with a third party library called ArUco, which is a well established computer vision library for c++ that helps with the calibration process. In the method we chose, the cameras are all centered around a stationary marker placed in the scene, so we required ArUco to detect these markers and calculate the camera's position relative to the marker. The aim is to find the transformation that brings points centered around the camera, into a space with the marker as the origin, then once this is calculated we can apply this transformation to each point cloud to align them. We start this process by detecting any markers visible in the scene, then finding rotation and translation vectors from the camera to this marker, adjusting for intrinsic camera parameters such as focal length and principal point offset. From this we can build up a transformation matrix to combine both the rotation and translation, before inverting it to get the translation from camera to marker as required.
\\

\subsubsection{Point cloud transformation}
Once we have found the transformation matrix as in the previous part, we must apply it to each point cloud before they are is sent to the server. We experimented with doing this by multiplying each xyz point by the transformation matrix to give the transformed point, but this was causing some performance issues due to the amount of points needed to be transformed at once. A better way of doing this would be to apply the transformation to all points in one operation with a more complex matrix operation. We used an optimised implementation of this in opencv called perspectiveTransform to do this efficiently, which when given the set of input points, and the transformation function we calculated earlier, would return the transformed points we needed. When each client calculated the transformation to bring their origin to the marker, and applied this transformation to the point clouds captured from the connect, they could be successfully combined such that they would align.
\end{document}
